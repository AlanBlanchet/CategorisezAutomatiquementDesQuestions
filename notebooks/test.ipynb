{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backlog\n",
    "\n",
    "- [X?] Le tester avec un petit modèle\n",
    "- [X] LDA + Topic analysis\n",
    "- [?] TFIDF + LogisticRegression, SGDClassifier\n",
    "- [X] BERT\n",
    "- [_] \n",
    "- [_] Target preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from cuml.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "import torch\n",
    "import cuml\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "plt.style.use(['ggplot', 'https://raw.githubusercontent.com/AlanBlanchet/matplotlib_styles/master/vscode_blue.mplstyle'])\n",
    "\n",
    "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\"\n",
    "\n",
    "RUN_ANIMATION_CELLS = False\n",
    "RUN_IGNORABLE = False\n",
    "RUN_ALL_DATA = False\n",
    "RUN_VIZ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_topics = Dataset(\"topics1\", n=5000)\n",
    "all_topics = Dataset(\"topics1\")\n",
    "topics = all_topics if RUN_ALL_DATA else short_topics\n",
    "topics.df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les types d'approches à tester :\n",
    "\n",
    "- Bag of Words (BoW) pour nous\n",
    "- Word Embeddings : Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itération 1\n",
    "\n",
    "Le but est de commencer à effectuer des prédictions le plus vite possible pour ensuite améliorer dans la prochaine itération. Ainsi, même si j'ai déjà remarquer quelque problèmes dans mon dataset, je vais faire des prédictions\n",
    "\n",
    "## Bag of Words - TF IDF\n",
    "\n",
    "Dans cet partie on va effectuer un bag of words avec tous les mots disponibles. \n",
    "\n",
    "On va ensuite appliquer l'algorithme du TF IDF afin d'obtenir nos vecteurs one hot encodés correspondant aux similitudes entre les titres. Ainsi on pourra visualiser la proximité des phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nltk.FreqDist()\n",
    "for t in topics.df[\"title\"]:\n",
    "    f.update(t.split(\" \"))\n",
    "print(len(f.items()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a maintenant des mots uniques à notre disposition pour effectuer nos prédictions. Or pour le moment on ne prédit rien. Commençons simplement par une visualisation des mots les plus importants de notre liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.quantile(sorted(f.values()), 0.25)\n",
    "to_remove = {k:v for k,v in f.items() if v <= q1}\n",
    "len(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.df[\"short_title\"] = topics.df[\"title\"].parallel_apply(lambda x: ' '.join([t for t in x.split(\" \") if t not in to_remove]))\n",
    "if RUN_IGNORABLE:\n",
    "    topics.df[\"short_title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3)\n",
    "\n",
    "labels = topics.df[\"target1\"]\n",
    "labels_counts = labels.value_counts()\n",
    "# Get top labels - also a memory saver\n",
    "q = np.quantile(labels_counts.values, 0.98)\n",
    "over_labels = labels_counts[labels_counts > q].index\n",
    "are_labels_in = labels.isin(over_labels)\n",
    "\n",
    "y_labels = labels[are_labels_in].reset_index(drop=True).to_numpy()\n",
    "y = topics.label2id(y_labels)\n",
    "X = topics.df.loc[are_labels_in,\"short_title\"].reset_index(drop=True)\n",
    "\n",
    "X = vectorizer.fit_transform(cudf.Series(X))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise d'abord le premier target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), np.array(y, dtype=np.float64), test_size=0.2, stratify=y)\n",
    "# Save memory\n",
    "X_train = cp.sparse.csr_matrix(cp.array(X_train))\n",
    "X_test = cp.sparse.csr_matrix(cp.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = naive_bayes.predict(X_test.toarray()).tolist()\n",
    "\n",
    "if RUN_VIZ:\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(topics.id2label(y_test.to_numpy()), topics.id2label(y_pred))\n",
    "    plt.title(\"< Q[0.98] target confusion matrix\")\n",
    "    plt.xticks(rotation=45, ha='right');\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VIZ:\n",
    "    print(classification_report(topics.id2label(y_test.to_numpy()), topics.id2label(y_pred), zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on remarque que beaucoup de prédictions sont faites pour le langage \"C#\" et que son score n'est pas terrible.\n",
    "\n",
    "Cela peut s'expliquer par le fait que le titre des questions soit trop générique ou présente un concept de code qui peut s'appliquer dans différents langage. Ex: \"Comment ajouter un élément à un tableau ?\"\n",
    "\n",
    "On ne pourrait donc pas déterminer le tag avec uniquement le titre.\n",
    "\n",
    "Visualisons ces données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tsne import tsne\n",
    "\n",
    "if RUN_ANIMATION_CELLS:\n",
    "    tsne(\"text_tsne\", X, y, topics.id2label(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons la même chose mais pour les descriptions (\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nltk.FreqDist()\n",
    "for t in topics.df[\"text\"]:\n",
    "    f.update(t.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nltk.FreqDist()\n",
    "for t in topics.df[\"text\"]:\n",
    "    f.update(t.split(\" \"))\n",
    "\n",
    "# Memory error if too much cols\n",
    "q = np.quantile(sorted(f.values()), 0.98)\n",
    "to_remove = {k:v for k,v in f.items() if v <= q}\n",
    "\n",
    "topics.df[\"short_text\"] = topics.df[\"text\"].parallel_apply(lambda x: ' '.join([t for t in x.split(\" \") if t not in to_remove]))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3)\n",
    "\n",
    "X = topics.df.loc[are_labels_in,\"short_text\"].reset_index(drop=True)\n",
    "X = vectorizer.fit_transform(cudf.Series(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), np.array(y, dtype=np.float64), test_size=0.2, stratify=y)\n",
    "X_train = cp.sparse.csr_matrix(cp.array(X_train))\n",
    "X_test = cp.sparse.csr_matrix(cp.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes.fit(X_train, y_train)\n",
    "y_pred = naive_bayes.predict(X_test.toarray()).tolist()\n",
    "\n",
    "if RUN_VIZ:\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(topics.id2label(y_test.to_numpy()), topics.id2label(y_pred))\n",
    "    plt.title(\"< Q[0.98] target confusion matrix\")\n",
    "    plt.xticks(rotation=45, ha='right');\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VIZ:\n",
    "    print(classification_report(topics.id2label(y_test.to_numpy()), topics.id2label(y_pred), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tsne import tsne\n",
    "\n",
    "if RUN_ANIMATION_CELLS:\n",
    "    tsne(\"text_tsne\", X, y, topics.id2label(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, Word2Vec, LdaSeqModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "topics.to(\"cpu\")\n",
    "docs = topics.df[\"short_text\"].str.split(\" \").values\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "len(dictionary), len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VIZ:\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        num_topics=20,\n",
    "        id2word=dictionary\n",
    "    )\n",
    "    vis_data = gensimvis.prepare(model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VIZ:\n",
    "    all_topics.to(\"cpu\")\n",
    "    docs = all_topics.df[\"short_text\"].str.split(\" \").values\n",
    "\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        num_topics=20,\n",
    "        id2word=dictionary\n",
    "    )\n",
    "\n",
    "    vis_data = gensimvis.prepare(model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Transform to the Dataset API\n",
    "data = topics.df.loc[are_labels_in.to_numpy()].reset_index(drop=True)\n",
    "data[\"target1\"] = topics.label2id(data[\"target1\"].values)\n",
    "dataset = datasets.Dataset.from_pandas(data[[\"title\", \"target1\"]])\n",
    "dataset = dataset.rename_columns({\"title\": \"text\", \"target1\": \"labels\"})\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=50)\n",
    "\n",
    "# PREPROCESS\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "print(dataset)\n",
    "dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels', 'token_type_ids'])\n",
    "\n",
    "# CREATE MODEL\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                        #    problem_type=\"label_classification\",\n",
    "                                                           num_labels=len(topics._id2label),\n",
    "                                                        #    id2label=topics._id2label,\n",
    "                                                        #    label2id=topics._label2id\n",
    "                                                           )\n",
    "\n",
    "fine_tune_name = model_name.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{fine_tune_name}-finetuned\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    # per_device_train_batch_size=4,\n",
    "    # per_device_eval_batch_size=2,\n",
    "    # no_cuda=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Get split datasets\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# clf_metrics = evaluate.combine([\"precision\", \"recall\"])\n",
    "clf_metrics = evaluate.load(\"precision\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_dataset[50]\n",
    "output = trainer.predict(test_dataset=[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics._id2label[sample[\"labels\"].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics._id2label[output[1].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
