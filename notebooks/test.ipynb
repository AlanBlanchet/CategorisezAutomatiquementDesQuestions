{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backlog\n",
    "\n",
    "- [X?] Le tester avec un petit modèle\n",
    "- [X] LDA + Topic analysis\n",
    "- [?] TFIDF + LogisticRegression, SGDClassifier\n",
    "- [X] BERT\n",
    "- [_] \n",
    "- [_] Target preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from cuml.naive_bayes import MultinomialNB\n",
    "from peft import LoraConfig, TaskType\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import torch\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from transformers import AutoTokenizer\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# 'https://raw.githubusercontent.com/AlanBlanchet/matplotlib_styles/master/vscode_blue.mplstyle'\n",
    "plt.style.use(['ggplot'])\n",
    "\n",
    "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\"\n",
    "\n",
    "RUN_ANIMATION_CELLS = False\n",
    "RUN_IGNORABLE = False\n",
    "RUN_ALL_DATA = True\n",
    "RUN_HEAVY = False\n",
    "RUN_VIZ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_topics = Dataset(\"topics1\", n=3500)\n",
    "all_topics = Dataset(\"topics1\")\n",
    "topics = all_topics if RUN_ALL_DATA else short_topics\n",
    "topics.df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les types d'approches à tester :\n",
    "\n",
    "- Bag of Words (BoW) pour nous\n",
    "- Word Embeddings : Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction - TF IDF\n",
    "\n",
    "Le but est de commencer à effectuer des prédictions le plus vite possible pour ensuite améliorer dans la prochaine itération. Ainsi, même si j'ai déjà remarquer quelque problèmes dans mon dataset, je vais faire des prédictions\n",
    "\n",
    "Dans cet partie on va effectuer un bag of words avec tous les mots disponibles. \n",
    "\n",
    "On va ensuite appliquer l'algorithme du TF IDF afin d'obtenir nos vecteurs one hot encodés correspondant aux similitudes entre les titres. Ainsi on pourra visualiser la proximité des phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nltk.FreqDist()\n",
    "for t in topics.df[\"title\"]:\n",
    "    f.update(t.split(\" \"))\n",
    "print(len(f.items()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a maintenant des mots uniques à notre disposition pour effectuer nos prédictions. Or pour le moment on ne prédit rien. Commençons simplement par une visualisation des mots les plus importants de notre liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.quantile(sorted(f.values()), 0.25)\n",
    "to_remove = {k:v for k,v in f.items() if v <= q1}\n",
    "len(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.df[\"short_title\"] = topics.df[\"title\"].parallel_apply(lambda x: ' '.join([t for t in x.split(\" \") if t not in to_remove]))\n",
    "if RUN_IGNORABLE:\n",
    "    topics.df[\"short_title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3)\n",
    "\n",
    "labels = topics.df[\"target1\"]\n",
    "labels_counts = labels.value_counts()\n",
    "# Get top labels - also a memory saver\n",
    "q = np.quantile(labels_counts.values, 0.98)\n",
    "over_labels = labels_counts[labels_counts > q].index\n",
    "are_labels_in = labels.isin(over_labels)\n",
    "\n",
    "y_labels = labels[are_labels_in].reset_index(drop=True).to_numpy()\n",
    "y = topics.label2id(y_labels)\n",
    "X = topics.df.loc[are_labels_in,\"short_title\"].reset_index(drop=True)\n",
    "\n",
    "X = vectorizer.fit_transform(cudf.Series(X))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise d'abord le premier target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), np.array(y, dtype=np.float64), test_size=0.2, stratify=y)\n",
    "# Save memory\n",
    "X_train = cp.sparse.csr_matrix(cp.array(X_train))\n",
    "X_test = cp.sparse.csr_matrix(cp.array(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = naive_bayes.predict(X_test.toarray()).tolist()\n",
    "\n",
    "if RUN_VIZ:\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(topics.id2label(y_test.to_numpy()), topics.id2label(y_pred))\n",
    "    plt.title(\"< Q[0.98] target confusion matrix\")\n",
    "    plt.xticks(rotation=45, ha='right');\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VIZ:\n",
    "    print(classification_report(topics.id2label(y_test.to_numpy()), topics.id2label(y_pred), zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on remarque que beaucoup de prédictions sont faites pour le langage \"C#\" et que son score n'est pas terrible.\n",
    "\n",
    "Cela peut s'expliquer par le fait que le titre des questions soit trop générique ou présente un concept de code qui peut s'appliquer dans différents langage. Ex: \"Comment ajouter un élément à un tableau ?\"\n",
    "\n",
    "On ne pourrait donc pas déterminer le tag avec uniquement le titre.\n",
    "\n",
    "Visualisons ces données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tsne import tsne, tsne_anim\n",
    "\n",
    "score = 0\n",
    "\n",
    "if RUN_ANIMATION_CELLS:\n",
    "    if RUN_HEAVY:\n",
    "       tsne_anim(\"title_tsne\", X[:10000].todense(), y[:10000], topics.id2label(y)[:10000])\n",
    "    else:\n",
    "        _, score = tsne(X[:10000].todense(), y[:10000], topics.id2label(y)[:10000])\n",
    "        del _\n",
    "        print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text\n",
    "\n",
    "Faisons la même chose mais pour les descriptions (\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nltk.FreqDist()\n",
    "for t in topics.df[\"text\"]:\n",
    "    f.update(t.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nltk.FreqDist()\n",
    "for t in topics.df[\"text\"]:\n",
    "    f.update(t.split(\" \"))\n",
    "\n",
    "# Memory error if too much cols\n",
    "q = np.quantile(sorted(f.values()), 0.98)\n",
    "to_remove = {k:v for k,v in f.items() if v <= q}\n",
    "\n",
    "topics.df[\"short_text\"] = topics.df[\"text\"].parallel_apply(lambda x: ' '.join([t for t in x.split(\" \") if t not in to_remove]))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3)\n",
    "\n",
    "X = topics.df.loc[are_labels_in,\"short_text\"].reset_index(drop=True)\n",
    "X = vectorizer.fit_transform(cudf.Series(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), np.array(y, dtype=np.float64), test_size=0.2, stratify=y)\n",
    "X_train = cp.sparse.csr_matrix(cp.array(X_train))\n",
    "X_test = cp.sparse.csr_matrix(cp.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes.fit(X_train, y_train)\n",
    "y_pred = naive_bayes.predict(X_test.toarray()).tolist()\n",
    "\n",
    "if RUN_VIZ:\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(topics.id2label(y_test.to_numpy()), topics.id2label(y_pred))\n",
    "    plt.title(\"< Q[0.98] target confusion matrix\")\n",
    "    plt.xticks(rotation=45, ha='right');\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VIZ:\n",
    "    print(classification_report(topics.id2label(y_test.to_numpy()), topics.id2label(y_pred), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "\n",
    "if RUN_ANIMATION_CELLS:\n",
    "    if RUN_HEAVY:\n",
    "        tsne_anim(\"text_tsne\", X[:10000].todense(), y[:10000], topics.id2label(y)[:10000])\n",
    "    else:\n",
    "        _, score = tsne(X.todense(), y, topics.id2label(y))\n",
    "        del _\n",
    "        print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA & Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "topics.to(\"cpu\")\n",
    "docs = topics.df[\"short_text\"].str.split(\" \").values\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "len(dictionary), len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VIZ:\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        num_topics=20,\n",
    "        id2word=dictionary\n",
    "    )\n",
    "    vis_data = gensimvis.prepare(model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VIZ:\n",
    "    all_topics.to(\"cpu\")\n",
    "    docs = all_topics.df[\"short_text\"].str.split(\" \").values\n",
    "\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        num_topics=20,\n",
    "        id2word=dictionary\n",
    "    )\n",
    "\n",
    "    vis_data = gensimvis.prepare(model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_size=300\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on title + text with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = topics.df[\"title\"] + topics.df[\"text\"]\n",
    "sentences_split = [sentence.split(\" \") for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(window=5, min_count=4, workers=10, vector_size=300, seed=0)\n",
    "w2v.build_vocab(sentences_split)\n",
    "w2v.train(sentences_split, total_examples=w2v.corpus_count, epochs=100)\n",
    "vecs = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque mot est assigné à un vecteur de dimension 300.\n",
    "\n",
    "On peut donc maintenant considérer que nos phrases sont elles-mêmes des embeddings avec un vecteur de dimension 300. Pour une phrase on prendra la moyenne de ses vecteurs words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = np.zeros((len(sentences_split), w2v_size))\n",
    "for i, sentence in enumerate(sentences_split):\n",
    "    vec = np.array([vecs[word] for word in sentence if word in vecs])\n",
    "    if len(vec) == 0:\n",
    "        continue\n",
    "    emb[i] = vec.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = emb\n",
    "y = topics.label2id(topics.df[\"target1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne(torch.tensor(X), y, topics.id2label(y), show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, jaccard_score\n",
    "\n",
    "model = LogisticRegression(n_jobs=8, random_state=0, max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "jaccard = jaccard_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f\"LogisticRegression {accuracy=} {jaccard=}\")\n",
    "\n",
    "model = SGDClassifier(n_jobs=8, max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "jaccard = jaccard_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f\"SGDClassifier {accuracy=} {jaccard=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to the Dataset API\n",
    "dataset = topics.to_datasets([\"target1\"], {\"target1\": \"labels\"}, tokenizer=tokenizer, sentence_length=128)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    inference_mode=False,\n",
    "    r=256,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    ")\n",
    "\n",
    "trainer = short_topics.trainer(model_name, dataset, peft=peft_config, batch_size=8, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reste du script est lancé directement avec l'interpréteur python car la mémoire GPU / RAM est instable avec les notebooks pour une raison qui m'est inconnue.\n",
    "\n",
    "Les fichiers des modèles sont présents dans src/run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
